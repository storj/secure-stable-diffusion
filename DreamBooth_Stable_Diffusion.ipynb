{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wnTMyW41cC1E",
    "tags": []
   },
   "source": [
    "## Hardware Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check type of GPU and VRAM available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XU7NuMAA2drw",
    "outputId": "7eb9b063-664f-4a42-e960-728ec9608c42",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Storj Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "STORJ_ACCESS = \"\" # Leave empty to use the Storj access grant configured as STORJ_ACCESS env var\n",
    "if not STORJ_ACCESS:\n",
    "    STORJ_ACCESS = os.getenv('STORJ_ACCESS')\n",
    "!uplink access inspect $STORJ_ACCESS > /dev/null && echo \"Will use this Storj access grant: $STORJ_ACCESS\" || echo \"Invalid Storj access grant: $STORJ_ACCESS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "STORJ_BUCKET = \"\" # Leave empty to use the Storj bucket configured as STORJ_BUCKET env var\n",
    "if not STORJ_BUCKET:\n",
    "    STORJ_BUCKET = os.getenv('STORJ_BUCKET')\n",
    "!uplink ls --access $STORJ_ACCESS sj://$STORJ_BUCKET > /dev/null && echo \"Will use this Storj bucket: $STORJ_BUCKET\" || echo \"Inaccessible Storj bucket: $STORJ_BUCKET\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training dataset location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instance_name = \"abc\"\n",
    "class_name = \"cat\"\n",
    "dataset_dir = f\"/workspace/data/{instance_name}\"\n",
    "!mkdir -p $dataset_dir\n",
    "STORJ_DATASET_PATH = \"dataset/512/\"\n",
    "\n",
    "if STORJ_DATASET_PATH:\n",
    "    !echo \"Downloading dataset from Storj\"\n",
    "    !uplink cp --access $STORJ_ACCESS --recursive sj://$STORJ_BUCKET/$STORJ_DATASET_PATH $dataset_dir\n",
    "\n",
    "#@markdown Run to generate a grid of preview images from the dataset.\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_image_grid(folder_path, columns):\n",
    "    image_files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "    num_images = len(image_files)\n",
    "    rows = num_images // columns + 1\n",
    "    images = []\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        image_path = os.path.join(folder_path, image_files[i])\n",
    "        image = Image.open(image_path)\n",
    "        images.append(image)\n",
    "    \n",
    "    fig = plt.figure(figsize=(16, 16))\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        plt.subplot(rows, columns, i+1)\n",
    "        plt.imshow(images[i])\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "display_image_grid(dataset_dir, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G0NV324ZcL9L"
   },
   "source": [
    "## Settings and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Rxg0y5MBudmd"
   },
   "outputs": [],
   "source": [
    "#@markdown Name/Path of the initial model.\n",
    "MODEL_NAME = \"runwayml/stable-diffusion-v1-5\" #@param {type:\"string\"}\n",
    "\n",
    "#@markdown Enter the directory name to save model at.\n",
    "\n",
    "OUTPUT_DIR = f\"/workspace/stable-diffusion-weights/{instance_name}\" #@param {type:\"string\"}\n",
    "\n",
    "print(f\"[*] Weights will be saved at {OUTPUT_DIR}\")\n",
    "\n",
    "!mkdir -p $OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5vDpCxId1aCm"
   },
   "outputs": [],
   "source": [
    "# You can also add multiple concepts here. Try tweaking `--max_train_steps` accordingly.\n",
    "\n",
    "concepts_list = [\n",
    "    {\n",
    "        \"instance_prompt\":      f\"photo of {instance_name} {class_name}\",\n",
    "        \"class_prompt\":         f\"photo of a {class_name}\",\n",
    "        \"instance_data_dir\":    dataset_dir,\n",
    "        \"class_data_dir\":       f\"/workspace/data/{class_name}\"\n",
    "    },\n",
    "]\n",
    "\n",
    "# `class_data_dir` contains regularization images\n",
    "import json\n",
    "import os\n",
    "for c in concepts_list:\n",
    "    os.makedirs(c[\"instance_data_dir\"], exist_ok=True)\n",
    "\n",
    "with open(\"concepts_list.json\", \"w\") as f:\n",
    "    json.dump(concepts_list, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/ShivamShrirao/diffusers\n",
    "%pip install -U --pre triton\n",
    "%pip install accelerate transformers ftfy bitsandbytes==0.35.0 gradio natsort safetensors xformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qn5ILIyDJIcX"
   },
   "source": [
    "# Start Training\n",
    "\n",
    "Use the table below to choose the best flags based on your memory and speed requirements. Tested on Tesla T4 GPU.\n",
    "\n",
    "\n",
    "| `fp16` | `train_batch_size` | `gradient_accumulation_steps` | `gradient_checkpointing` | `use_8bit_adam` | GB VRAM usage | Speed (it/s) |\n",
    "| ---- | ------------------ | ----------------------------- | ----------------------- | --------------- | ---------- | ------------ |\n",
    "| fp16 | 1                  | 1                             | TRUE                    | TRUE            | 9.92       | 0.93         |\n",
    "| no   | 1                  | 1                             | TRUE                    | TRUE            | 10.08      | 0.42         |\n",
    "| fp16 | 2                  | 1                             | TRUE                    | TRUE            | 10.4       | 0.66         |\n",
    "| fp16 | 1                  | 1                             | FALSE                   | TRUE            | 11.17      | 1.14         |\n",
    "| no   | 1                  | 1                             | FALSE                   | TRUE            | 11.17      | 0.49         |\n",
    "| fp16 | 1                  | 2                             | TRUE                    | TRUE            | 11.56      | 1            |\n",
    "| fp16 | 2                  | 1                             | FALSE                   | TRUE            | 13.67      | 0.82         |\n",
    "| fp16 | 1                  | 2                             | FALSE                   | TRUE            | 13.7       | 0.83          |\n",
    "| fp16 | 1                  | 1                             | TRUE                    | FALSE           | 15.79      | 0.77         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ioxxvHoicPs"
   },
   "source": [
    "Add `--gradient_checkpointing` flag for around 9.92 GB VRAM usage.\n",
    "\n",
    "remove `--use_8bit_adam` flag for full precision. Requires 15.79 GB with `--gradient_checkpointing` else 17.8 GB.\n",
    "\n",
    "remove `--train_text_encoder` flag to reduce memory usage further, degrades output quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jjcSXTp-u-Eg"
   },
   "outputs": [],
   "source": [
    "!python3 dreambooth/train_dreambooth.py \\\n",
    "  --pretrained_model_name_or_path=$MODEL_NAME \\\n",
    "  --pretrained_vae_name_or_path=\"stabilityai/sd-vae-ft-mse\" \\\n",
    "  --output_dir=$OUTPUT_DIR \\\n",
    "  --revision=\"fp16\" \\\n",
    "  --with_prior_preservation --prior_loss_weight=1.0 \\\n",
    "  --seed=1337 \\\n",
    "  --resolution=512 \\\n",
    "  --train_batch_size=1 \\\n",
    "  --train_text_encoder \\\n",
    "  --mixed_precision=\"fp16\" \\\n",
    "  --use_8bit_adam \\\n",
    "  --gradient_accumulation_steps=1 \\\n",
    "  --learning_rate=1e-6 \\\n",
    "  --lr_scheduler=\"constant\" \\\n",
    "  --lr_warmup_steps=0 \\\n",
    "  --num_class_images=50 \\\n",
    "  --sample_batch_size=4 \\\n",
    "  --max_train_steps=800 \\\n",
    "  --save_interval=10000 \\\n",
    "  --save_sample_prompt=f\"photo of {instance_name} {class_name}\" \\\n",
    "  --concepts_list=\"concepts_list.json\"\n",
    "\n",
    "# Reduce the `--save_interval` to lower than `--max_train_steps` to save weights from intermediate steps.\n",
    "# `--save_sample_prompt` can be same as `--instance_prompt` to generate intermediate samples (saved along with weights in samples directory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "89Az5NUxOWdy"
   },
   "outputs": [],
   "source": [
    "#@markdown Specify the weights directory to use (leave blank for latest)\n",
    "WEIGHTS_DIR = \"\" #@param {type:\"string\"}\n",
    "if WEIGHTS_DIR == \"\":\n",
    "    from natsort import natsorted\n",
    "    from glob import glob\n",
    "    import os\n",
    "    WEIGHTS_DIR = natsorted(glob(OUTPUT_DIR + os.sep + \"*\"))[-1]\n",
    "print(f\"[*] WEIGHTS_DIR={WEIGHTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "89Az5NUxOWdy"
   },
   "outputs": [],
   "source": [
    "#@markdown Run to generate a grid of preview images from the last saved weights.\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "weights_folder = OUTPUT_DIR\n",
    "folders = sorted([f for f in os.listdir(weights_folder) if f != \"0\"], key=lambda x: int(x))\n",
    "\n",
    "row = len(folders)\n",
    "col = len(os.listdir(os.path.join(weights_folder, folders[0], \"samples\")))\n",
    "scale = 4\n",
    "fig, axes = plt.subplots(row, col, figsize=(col*scale, row*scale), gridspec_kw={'hspace': 0, 'wspace': 0})\n",
    "\n",
    "for i, folder in enumerate(folders):\n",
    "    folder_path = os.path.join(weights_folder, folder)\n",
    "    image_folder = os.path.join(folder_path, \"samples\")\n",
    "    images = [f for f in os.listdir(image_folder)]\n",
    "    for j, image in enumerate(images):\n",
    "        if row == 1:\n",
    "            currAxes = axes[j]\n",
    "        else:\n",
    "            currAxes = axes[i, j]\n",
    "        if i == 0:\n",
    "            currAxes.set_title(f\"Image {j}\")\n",
    "        if j == 0:\n",
    "            currAxes.text(-0.1, 0.5, folder, rotation=0, va='center', ha='center', transform=currAxes.transAxes)\n",
    "        image_path = os.path.join(image_folder, image)\n",
    "        img = mpimg.imread(image_path)\n",
    "        currAxes.imshow(img, cmap='gray')\n",
    "        currAxes.axis('off')\n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.savefig('grid.png', dpi=72)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5V8wgU0HN-Kq"
   },
   "source": [
    "## Convert weights to ckpt to use in web UIs like AUTOMATIC1111."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "dcXzsUyG1aCy"
   },
   "outputs": [],
   "source": [
    "#@markdown Run conversion.\n",
    "steps = os.path.basename(WEIGHTS_DIR)\n",
    "ckpt_path = f\"/workspace/stable-diffusion-webui/models/Stable-diffusion/sd15-{instance_name}-{class_name}-{steps}.ckpt\"\n",
    "\n",
    "half_arg = \"\"\n",
    "#@markdown  Whether to convert to fp16, takes half the space (2GB).\n",
    "fp16 = True #@param {type: \"boolean\"}\n",
    "if fp16:\n",
    "    half_arg = \"--half\"\n",
    "!python dreambooth/convert_diffusers_to_original_stable_diffusion.py --model_path $WEIGHTS_DIR  --checkpoint_path $ckpt_path $half_arg\n",
    "print(f\"[*] Converted ckpt saved at {ckpt_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ToNG4fd_dTbF"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gW15FjffdTID"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import autocast\n",
    "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
    "from IPython.display import display\n",
    "\n",
    "model_path = WEIGHTS_DIR             # If you want to use previously trained model saved in gdrive, replace this with the full path of model in gdrive\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_path, safety_checker=None, torch_dtype=torch.float16).to(\"cuda\")\n",
    "pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
    "pipe.enable_xformers_memory_efficient_attention()\n",
    "g_cuda = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oIzkltjpVO_f",
    "outputId": "1db9fcaa-2d0f-4966-dc4f-baac60cdb807"
   },
   "outputs": [],
   "source": [
    "#@markdown Can set random seed here for reproducibility.\n",
    "g_cuda = torch.Generator(device='cuda')\n",
    "seed = 52362 #@param {type:\"number\"}\n",
    "g_cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "referenced_widgets": [
      "21d3153693b0442bb0afe46738c9d9ae"
     ]
    },
    "id": "K6xoHWSsbcS3",
    "outputId": "75fb2672-a0a4-4149-ef0d-42ff0c247449"
   },
   "outputs": [],
   "source": [
    "#@title Run for generating images.\n",
    "\n",
    "prompt = f\"photo of {instance_name} {class_name} in a bucket\" #@param {type:\"string\"}\n",
    "negative_prompt = \"\" #@param {type:\"string\"}\n",
    "num_samples = 4 #@param {type:\"number\"}\n",
    "guidance_scale = 7.5 #@param {type:\"number\"}\n",
    "num_inference_steps = 24 #@param {type:\"number\"}\n",
    "height = 512 #@param {type:\"number\"}\n",
    "width = 512 #@param {type:\"number\"}\n",
    "\n",
    "with autocast(\"cuda\"), torch.inference_mode():\n",
    "    images = pipe(\n",
    "        prompt,\n",
    "        height=height,\n",
    "        width=width,\n",
    "        negative_prompt=negative_prompt,\n",
    "        num_images_per_prompt=num_samples,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        guidance_scale=guidance_scale,\n",
    "        generator=g_cuda\n",
    "    ).images\n",
    "\n",
    "for img in images:\n",
    "    display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Refresh model list in Stable Diffusion Web UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!curl https://$RUNPOD_POD_ID-7860.proxy.runpod.net/sdapi/v1/refresh-checkpoints  -d ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload trained model to Storj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "STORJ_MODEL_PATH = \"\" # Leave empty to use the model path configured as STORJ_MODEL_PATH env var\n",
    "if not STORJ_MODEL_PATH:\n",
    "    STORJ_MODEL_PATH = os.getenv('STORJ_MODEL_PATH')\n",
    "!uplink cp --access $STORJ_ACCESS --parallelism 16 $ckpt_path sj://$STORJ_BUCKET/$STORJ_MODEL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jXgi8HM4c-DA"
   },
   "outputs": [],
   "source": [
    "#@title Free runtime memory\n",
    "exit()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
