{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will guide you through fine-tuning a Stable Diffusion model with [DreamBooth](https://dreambooth.github.io/).\n",
    "\n",
    "Unlike similar notebooks, this one focuses on securely fetching the training dataset from a Storj bucket and securely uploading the trained checkpoints back to the bucket for future use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wnTMyW41cC1E",
    "tags": []
   },
   "source": [
    "## Hardware Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the type of GPU and VRAM available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XU7NuMAA2drw",
    "outputId": "7eb9b063-664f-4a42-e960-728ec9608c42",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/ShivamShrirao/diffusers\n",
    "%pip install -U --pre triton\n",
    "%pip install accelerate transformers ftfy bitsandbytes==0.35.0 gradio natsort safetensors xformers matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Storj Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the [access grant](https://docs.storj.io/dcs/concepts/access/access-grants) to the Storj bucket that contains the training dataset and where trained checkpoints will be uploaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "######### SET THE VALUE OF THE VARIABLE BELOW #########\n",
    "\n",
    "STORJ_ACCESS = \"\" # Leave empty to use the Storj access grant configured as STORJ_ACCESS env var\n",
    "\n",
    "######### DO NOT MODIFY THE CODE BELOW #########\n",
    "\n",
    "import os\n",
    "if not STORJ_ACCESS:\n",
    "    STORJ_ACCESS = os.getenv('STORJ_ACCESS')\n",
    "!uplink access inspect $STORJ_ACCESS --interactive=false > /dev/null && echo \"Will use this Storj access grant: $STORJ_ACCESS\" || echo \"Invalid Storj access grant: $STORJ_ACCESS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "######### SET THE VALUE OF THE VARIABLE BELOW #########\n",
    "\n",
    "STORJ_BUCKET = \"\" # Leave empty to use the Storj bucket configured as STORJ_BUCKET env var\n",
    "\n",
    "######### DO NOT MODIFY THE CODE BELOW #########\n",
    "\n",
    "if not STORJ_BUCKET:\n",
    "    STORJ_BUCKET = os.getenv('STORJ_BUCKET')\n",
    "!uplink ls --access $STORJ_ACCESS --interactive=false sj://$STORJ_BUCKET > /dev/null && echo \"Will use this Storj bucket: $STORJ_BUCKET\" || echo \"Inaccessible Storj bucket: $STORJ_BUCKET\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose the rare unique identifier and class name for the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### SET THE VALUE OF THE VARIABLEs BELOW #########\n",
    "\n",
    "UNIQUE_IDENTIFIER = \"clgr\" # Should be a rare string of characters, not a real word\n",
    "CLASS_NAME = \"cat\" # The subject class the unique identifier relates to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the path to the input images relative to the root of the Storj bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "######### SET THE VALUE OF THE VARIABLE BELOW #########\n",
    "\n",
    "STORJ_DATASET_PATH = \"path/to/the/dataset/\"\n",
    "\n",
    "######### DO NOT MODIFY THE CODE BELOW #########\n",
    "\n",
    "dataset_dir = f\"/workspace/data/{UNIQUE_IDENTIFIER}\"\n",
    "!mkdir -p $dataset_dir\n",
    "\n",
    "if STORJ_DATASET_PATH:\n",
    "    !echo \"Downloading dataset from Storj\"\n",
    "    !uplink cp --access $STORJ_ACCESS --recursive sj://$STORJ_BUCKET/$STORJ_DATASET_PATH $dataset_dir\n",
    "\n",
    "# generates a grid of preview images from the dataset.\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_image_grid(folder_path, columns):\n",
    "    image_files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "    num_images = len(image_files)\n",
    "    rows = num_images // columns + 1\n",
    "    images = []\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        image_path = os.path.join(folder_path, image_files[i])\n",
    "        image = Image.open(image_path)\n",
    "        images.append(image)\n",
    "    \n",
    "    fig = plt.figure(figsize=(16, 16))\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        plt.subplot(rows, columns, i+1)\n",
    "        plt.imshow(images[i])\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "display_image_grid(dataset_dir, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G0NV324ZcL9L"
   },
   "source": [
    "## Choose the base model for the training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If unsure, leave it as it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Rxg0y5MBudmd"
   },
   "outputs": [],
   "source": [
    "######### SET THE VALUE OF THE VARIABLE BELOW #########\n",
    "\n",
    "MODEL_NAME = \"runwayml/stable-diffusion-v1-5\"\n",
    "\n",
    "######### DO NOT MODIFY THE CODE BELOW #########\n",
    "\n",
    "OUTPUT_DIR = f\"/workspace/stable-diffusion-weights/{UNIQUE_IDENTIFIER}\"\n",
    "\n",
    "print(f\"[*] Weights will be saved at {OUTPUT_DIR}\")\n",
    "\n",
    "!mkdir -p $OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5vDpCxId1aCm"
   },
   "outputs": [],
   "source": [
    "######### DO NOT MODIFY THE CODE BELOW #########\n",
    "\n",
    "concepts_list = [\n",
    "    {\n",
    "        \"instance_prompt\":      f\"photo of {UNIQUE_IDENTIFIER} {CLASS_NAME}\",\n",
    "        \"class_prompt\":         f\"photo of a {CLASS_NAME}\",\n",
    "        \"instance_data_dir\":    dataset_dir,\n",
    "        \"class_data_dir\":       f\"/workspace/data/{CLASS_NAME}\"\n",
    "    },\n",
    "]\n",
    "\n",
    "# `class_data_dir` contains regularization images\n",
    "import json\n",
    "import os\n",
    "for c in concepts_list:\n",
    "    os.makedirs(c[\"instance_data_dir\"], exist_ok=True)\n",
    "\n",
    "with open(\"dreambooth/concepts_list.json\", \"w\") as f:\n",
    "    json.dump(concepts_list, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qn5ILIyDJIcX"
   },
   "source": [
    "# Start Training\n",
    "\n",
    "Use the table below to choose the best flags based on your memory and speed requirements. Tested on Tesla T4 GPU.\n",
    "\n",
    "\n",
    "| `fp16` | `train_batch_size` | `gradient_accumulation_steps` | `gradient_checkpointing` | `use_8bit_adam` | GB VRAM usage | Speed (it/s) |\n",
    "| ---- | ------------------ | ----------------------------- | ----------------------- | --------------- | ---------- | ------------ |\n",
    "| fp16 | 1                  | 1                             | TRUE                    | TRUE            | 9.92       | 0.93         |\n",
    "| no   | 1                  | 1                             | TRUE                    | TRUE            | 10.08      | 0.42         |\n",
    "| fp16 | 2                  | 1                             | TRUE                    | TRUE            | 10.4       | 0.66         |\n",
    "| fp16 | 1                  | 1                             | FALSE                   | TRUE            | 11.17      | 1.14         |\n",
    "| no   | 1                  | 1                             | FALSE                   | TRUE            | 11.17      | 0.49         |\n",
    "| fp16 | 1                  | 2                             | TRUE                    | TRUE            | 11.56      | 1            |\n",
    "| fp16 | 2                  | 1                             | FALSE                   | TRUE            | 13.67      | 0.82         |\n",
    "| fp16 | 1                  | 2                             | FALSE                   | TRUE            | 13.7       | 0.83          |\n",
    "| fp16 | 1                  | 1                             | TRUE                    | FALSE           | 15.79      | 0.77         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ioxxvHoicPs"
   },
   "source": [
    "Add `--gradient_checkpointing` flag for around 9.92 GB VRAM usage.\n",
    "\n",
    "remove `--use_8bit_adam` flag for full precision. Requires 15.79 GB with `--gradient_checkpointing` else 17.8 GB.\n",
    "\n",
    "remove `--train_text_encoder` flag to reduce memory usage further, degrades output quality.\n",
    "\n",
    "If unsure, leave everything as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jjcSXTp-u-Eg"
   },
   "outputs": [],
   "source": [
    "!python3 dreambooth/train_dreambooth.py \\\n",
    "  --pretrained_model_name_or_path=$MODEL_NAME \\\n",
    "  --pretrained_vae_name_or_path=\"stabilityai/sd-vae-ft-mse\" \\\n",
    "  --output_dir=$OUTPUT_DIR \\\n",
    "  --revision=\"fp16\" \\\n",
    "  --with_prior_preservation --prior_loss_weight=1.0 \\\n",
    "  --seed=1337 \\\n",
    "  --resolution=512 \\\n",
    "  --train_batch_size=1 \\\n",
    "  --train_text_encoder \\\n",
    "  --mixed_precision=\"fp16\" \\\n",
    "  --use_8bit_adam \\\n",
    "  --gradient_accumulation_steps=1 \\\n",
    "  --learning_rate=1e-6 \\\n",
    "  --lr_scheduler=\"constant\" \\\n",
    "  --lr_warmup_steps=0 \\\n",
    "  --num_class_images=50 \\\n",
    "  --sample_batch_size=4 \\\n",
    "  --max_train_steps=800 \\\n",
    "  --save_interval=10000 \\\n",
    "  --save_sample_prompt=f\"photo of {UNIQUE_IDENTIFIER} {CLASS_NAME}\" \\\n",
    "  --concepts_list=\"dreambooth/concepts_list.json\"\n",
    "\n",
    "# Reduce the `--save_interval` to lower than `--max_train_steps` to save weights from intermediate steps.\n",
    "# `--save_sample_prompt` can be same as `--instance_prompt` to generate intermediate samples (saved along with weights in samples directory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "89Az5NUxOWdy"
   },
   "outputs": [],
   "source": [
    "######### DO NOT MODIFY THE CODE BELOW #########\n",
    "\n",
    "WEIGHTS_DIR = \"\"\n",
    "if WEIGHTS_DIR == \"\":\n",
    "    from natsort import natsorted\n",
    "    from glob import glob\n",
    "    import os\n",
    "    WEIGHTS_DIR = natsorted(glob(OUTPUT_DIR + os.sep + \"*\"))[-1]\n",
    "print(f\"[*] WEIGHTS_DIR={WEIGHTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a grid of preview images from the last saved weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "89Az5NUxOWdy"
   },
   "outputs": [],
   "source": [
    "######### DO NOT MODIFY THE CODE BELOW #########\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "weights_folder = OUTPUT_DIR\n",
    "folders = sorted([f for f in os.listdir(weights_folder) if f != \"0\"], key=lambda x: int(x))\n",
    "\n",
    "row = len(folders)\n",
    "col = len(os.listdir(os.path.join(weights_folder, folders[0], \"samples\")))\n",
    "scale = 4\n",
    "fig, axes = plt.subplots(row, col, figsize=(col*scale, row*scale), gridspec_kw={'hspace': 0, 'wspace': 0})\n",
    "\n",
    "for i, folder in enumerate(folders):\n",
    "    folder_path = os.path.join(weights_folder, folder)\n",
    "    image_folder = os.path.join(folder_path, \"samples\")\n",
    "    images = [f for f in os.listdir(image_folder)]\n",
    "    for j, image in enumerate(images):\n",
    "        if row == 1:\n",
    "            currAxes = axes[j]\n",
    "        else:\n",
    "            currAxes = axes[i, j]\n",
    "        if i == 0:\n",
    "            currAxes.set_title(f\"Image {j}\")\n",
    "        if j == 0:\n",
    "            currAxes.text(-0.1, 0.5, folder, rotation=0, va='center', ha='center', transform=currAxes.transAxes)\n",
    "        image_path = os.path.join(image_folder, image)\n",
    "        img = mpimg.imread(image_path)\n",
    "        currAxes.imshow(img, cmap='gray')\n",
    "        currAxes.axis('off')\n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.savefig('dreambooth/grid.png', dpi=72)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5V8wgU0HN-Kq"
   },
   "source": [
    "## Convert weights to checkpoints to use in Stable Diffusion Web UIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "dcXzsUyG1aCy"
   },
   "outputs": [],
   "source": [
    "######### DO NOT MODIFY THE CODE BELOW #########\n",
    "\n",
    "steps = os.path.basename(WEIGHTS_DIR)\n",
    "ckpt_path = f\"/workspace/stable-diffusion-webui/models/Stable-diffusion/sd15-{UNIQUE_IDENTIFIER}-{CLASS_NAME}-{steps}.ckpt\"\n",
    "\n",
    "half_arg = \"\"\n",
    "# Whether to convert to fp16, takes half the space (2GB).\n",
    "fp16 = True\n",
    "if fp16:\n",
    "    half_arg = \"--half\"\n",
    "!python dreambooth/convert_diffusers_to_original_stable_diffusion.py --model_path $WEIGHTS_DIR  --checkpoint_path $ckpt_path $half_arg\n",
    "print(f\"[*] Converted ckpt saved at {ckpt_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ToNG4fd_dTbF"
   },
   "source": [
    "## Test inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gW15FjffdTID"
   },
   "outputs": [],
   "source": [
    "######### DO NOT MODIFY THE CODE BELOW #########\n",
    "\n",
    "import torch\n",
    "from torch import autocast\n",
    "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
    "from IPython.display import display\n",
    "\n",
    "model_path = WEIGHTS_DIR\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_path, safety_checker=None, torch_dtype=torch.float16).to(\"cuda\")\n",
    "pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
    "pipe.enable_xformers_memory_efficient_attention()\n",
    "g_cuda = None\n",
    "\n",
    "# Can set random seed here for reproducibility.\n",
    "g_cuda = torch.Generator(device='cuda')\n",
    "seed = 52362\n",
    "g_cuda.manual_seed(seed)\n",
    "\n",
    "# Generate the images.\n",
    "\n",
    "prompt = f\"photo of {UNIQUE_IDENTIFIER} {CLASS_NAME} in a bucket\"\n",
    "negative_prompt = \"\"\n",
    "num_samples = 4\n",
    "guidance_scale = 7.5\n",
    "num_inference_steps = 24\n",
    "height = 512\n",
    "width = 512\n",
    "\n",
    "with autocast(\"cuda\"), torch.inference_mode():\n",
    "    images = pipe(\n",
    "        prompt,\n",
    "        height=height,\n",
    "        width=width,\n",
    "        negative_prompt=negative_prompt,\n",
    "        num_images_per_prompt=num_samples,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        guidance_scale=guidance_scale,\n",
    "        generator=g_cuda\n",
    "    ).images\n",
    "\n",
    "for img in images:\n",
    "    display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Refresh model list in Stable Diffusion Web UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!curl https://$RUNPOD_POD_ID-7860.proxy.runpod.net/sdapi/v1/refresh-checkpoints  -d ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload trained model to Storj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the path to upload the trained checkpoint relative to the root of the Storj bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "######### SET THE VALUE OF THE VARIABLE BELOW #########\n",
    "\n",
    "STORJ_MODEL_PATH = \"\" # Leave empty to use the model path configured as STORJ_MODEL_PATH env var\n",
    "\n",
    "######### DO NOT MODIFY THE CODE BELOW #########\n",
    "\n",
    "import os\n",
    "if not STORJ_MODEL_PATH:\n",
    "    STORJ_MODEL_PATH = os.getenv('STORJ_MODEL_PATH')\n",
    "\n",
    "if STORJ_MODEL_PATH:\n",
    "    !uplink cp --access $STORJ_ACCESS --parallelism 16 $ckpt_path sj://$STORJ_BUCKET/$STORJ_MODEL_PATH\n",
    "else:\n",
    "    print(\"[*] Path not set, won't upload the checkpoint.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
